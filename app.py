import streamlit as st
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
from pathlib import Path

@st.cache_resource
def load_model():
    APP_DIR = Path(__file__).parent
    MODEL_PATH = APP_DIR / "llm_winner_classifier"
    
    model = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH)
    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
    return model, tokenizer

st.title("ğŸ¤– AI vs. Human Text Detector")
st.write("Bu uygulama, girdiÄŸiniz metnin bir yapay zeka tarafÄ±ndan mÄ± yoksa bir insan tarafÄ±ndan mÄ± yazÄ±ldÄ±ÄŸÄ±nÄ± tahmin eder.")

try:
    model, tokenizer = load_model()
    user_input = st.text_area("LÃ¼tfen analiz edilecek metni buraya girin:", height=150)

    if st.button("Metni Analiz Et"):
        if user_input:
            inputs = tokenizer(user_input, return_tensors="pt", truncation=True, padding=True, max_length=512)
            
            with torch.no_grad():
                logits = model(**inputs).logits
            
            predicted_class_id = torch.argmax(logits, dim=1).item()
            
            if predicted_class_id == 1:
                st.success("Tahmin: Bu metin bÃ¼yÃ¼k ihtimalle **Yapay Zeka** tarafÄ±ndan Ã¼retilmiÅŸtir. ğŸ¤–")
            else:
                st.info("Tahmin: Bu metin bÃ¼yÃ¼k ihtimalle bir **Ä°nsan** tarafÄ±ndan yazÄ±lmÄ±ÅŸtÄ±r. ğŸ§‘â€ğŸ’»")
        else:
            st.warning("LÃ¼tfen analiz etmek iÃ§in bir metin girin.")

except Exception as e:
    st.error(f"Model yÃ¼klenirken bir hata oluÅŸtu: {e}")
    st.info("LÃ¼tfen 'llm_winner_classifier' klasÃ¶rÃ¼nÃ¼n app.py ile aynÄ± dizinde olduÄŸundan emin olun.")
