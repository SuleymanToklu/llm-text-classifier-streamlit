import streamlit as st
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

st.set_page_config(
    page_title="LLM KarÅŸÄ±laÅŸtÄ±rÄ±cÄ±",
    page_icon="ğŸ†",
    layout="wide"
)

MODEL_PATH = "./llm_winner_classifier"

@st.cache_resource
def load_model():
    try:
        model = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH)
        tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
        return model, tokenizer
    except OSError:
        return None, None

model, tokenizer = load_model()

st.sidebar.title("â„¹ï¸ NasÄ±l KullanÄ±lÄ±r?")
st.sidebar.markdown(
    """
    Bu uygulama, iki farklÄ± yapay zeka modelinin cevaplarÄ±nÄ± karÅŸÄ±laÅŸtÄ±rmak iÃ§in tasarlanmÄ±ÅŸtÄ±r.

    **AdÄ±mlar:**
    1.  **Bir komut (prompt) belirleyin.**
        *(Ã–rn: "Ä°stanbul'da gezilecek 5 yer Ã¶nerir misin?*")

    2.  Bu komutu, sonuÃ§larÄ±nÄ± merak ettiÄŸiniz **iki farklÄ± yapay zeka modeline** sorun.
        *(Ã–rn: ChatGPT, Gemini, Claude vb.)*

    3.  AldÄ±ÄŸÄ±nÄ±z iki farklÄ± cevabÄ± aÅŸaÄŸÄ±daki **'Cevap A'** ve **'Cevap B'** kutularÄ±na yapÄ±ÅŸtÄ±rÄ±n.

    4.  **'Analiz Et ve KazananÄ± Bul!'** butonuna tÄ±klayarak modelimizin tahminini gÃ¶rÃ¼n.
    """
)

st.sidebar.title("ğŸ¤– Model HakkÄ±nda")
st.sidebar.info(
    "Bu uygulama, `distilbert-base-uncased` modelinin Kaggle'daki bir veri seti ile "
    "fine-tune edilmiÅŸ halini kullanmaktadÄ±r. AmaÃ§, iki metin arasÄ±ndaki 'daha iyi' olanÄ± belirlemektir."
)

st.title("ğŸ† LLM Cevap KarÅŸÄ±laÅŸtÄ±rÄ±cÄ±")
st.markdown("Ä°ki farklÄ± yapay zeka cevabÄ±nÄ± analiz ederek hangisinin daha baÅŸarÄ±lÄ± olduÄŸunu tahmin edin.")
st.markdown("---")

if model is None or tokenizer is None:
    st.error(
        f"**Model yÃ¼klenemedi!** LÃ¼tfen `train_model.py` betiÄŸini Ã§alÄ±ÅŸtÄ±rdÄ±ÄŸÄ±nÄ±zdan ve '{MODEL_PATH}' "
        "klasÃ¶rÃ¼nÃ¼n mevcut olduÄŸundan emin olun."
    )
else:
    st.subheader("1. AdÄ±m: KarÅŸÄ±laÅŸtÄ±rma Komutunuz")
    prompt_text = st.text_area(
        "KarÅŸÄ±laÅŸtÄ±rma iÃ§in temel alÄ±nacak komutu buraya girin:",
        height=100,
        placeholder="Ã–rn: Ä°klim deÄŸiÅŸikliÄŸinin ana nedenleri nelerdir?"
    )

    st.subheader("2. AdÄ±m: Yapay Zeka CevaplarÄ±")
    col1, col2 = st.columns(2)
    with col1:
        response_a = st.text_area(
            "Cevap A",
            height=250,
            placeholder="Ä°lk yapay zeka modelinden aldÄ±ÄŸÄ±nÄ±z cevabÄ± buraya yapÄ±ÅŸtÄ±rÄ±n."
        )
    with col2:
        response_b = st.text_area(
            "Cevap B",
            height=250,
            placeholder="Ä°kinci yapay zeka modelinden aldÄ±ÄŸÄ±nÄ±z cevabÄ± buraya yapÄ±ÅŸtÄ±rÄ±n."
        )

    st.markdown("---")

    if st.button("Analiz Et ve KazananÄ± Bul!", type="primary", use_container_width=True):
        if all([prompt_text, response_a, response_b]):
            with st.spinner("ğŸ§  Analiz yapÄ±lÄ±yor..."):
                sep_token = tokenizer.sep_token
                input_text = prompt_text + sep_token + response_a + sep_token + response_b

                inputs = tokenizer(input_text, return_tensors="pt", truncation=True, padding="max_length", max_length=512)

                with torch.no_grad():
                    logits = model(**inputs).logits

                predicted_class_id = torch.argmax(logits, dim=1).item()

                st.subheader("âœ¨ SonuÃ§")
                if predicted_class_id == 0:
                    st.success("Kazanan: **Cevap A** daha baÅŸarÄ±lÄ± gÃ¶rÃ¼nÃ¼yor!", icon="ğŸ‡¦")
                elif predicted_class_id == 1:
                    st.success("Kazanan: **Cevap B** daha baÅŸarÄ±lÄ± gÃ¶rÃ¼nÃ¼yor!", icon="ğŸ‡§")
                else:
                    st.info("SonuÃ§: Ä°ki cevap arasÄ±nda belirgin bir fark bulunamadÄ± (**Berabere**).", icon="ğŸ¤")
                
                st.balloons()
        else:
            st.warning("LÃ¼tfen karÅŸÄ±laÅŸtÄ±rma yapmak iÃ§in tÃ¼m alanlarÄ± doldurun.", icon="âš ï¸")